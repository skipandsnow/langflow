"use strict";(self.webpackChunklangflow_docs=self.webpackChunklangflow_docs||[]).push([[9366],{6462:(e,n,o)=>{o.r(n),o.d(n,{CH:()=>h,assets:()=>a,chCodeConfig:()=>p,contentTitle:()=>c,default:()=>g,frontMatter:()=>r,metadata:()=>d,toc:()=>x});o(6540);var s=o(4848),t=o(8453),l=o(4754),i=o(7293);const r={},c="Models",d={id:"components/models",title:"Models",description:"This page may contain outdated information. It will be updated as soon as possible.",source:"@site/docs/components/models.mdx",sourceDirName:"components",slug:"/components/models",permalink:"/components/models",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{},sidebar:"docs",previous:{title:"Data",permalink:"/components/data"},next:{title:"Helpers",permalink:"/components/helpers"}},a={},h={annotations:l.hk,InlineCode:l.R0},p={staticMediaQuery:"not screen, (max-width: 768px)",lineNumbers:!0,showCopyButton:!0,themeName:"github-dark"},x=[{value:"Amazon Bedrock",id:"amazon-bedrock",level:2},{value:"Anthropic",id:"anthropic",level:2},{value:"Azure OpenAI",id:"azure-openai",level:2},{value:"Cohere",id:"cohere",level:2},{value:"Google Generative AI",id:"google-generative-ai",level:2},{value:"Hugging Face API",id:"hugging-face-api",level:2},{value:"LiteLLM Model",id:"litellm-model",level:2},{value:"Ollama",id:"ollama",level:2},{value:"OpenAI",id:"openai",level:2},{value:"Qianfan",id:"qianfan",level:2},{value:"Vertex AI",id:"vertex-ai",level:2}];function j(e){const n=Object.assign({h1:"h1",p:"p",h2:"h2",strong:"strong",ul:"ul",li:"li",hr:"hr",a:"a",code:"code"},(0,t.RP)(),e.components);return h||u("CH",!1),h.InlineCode||u("CH.InlineCode",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)("style",{dangerouslySetInnerHTML:{__html:'[data-ch-theme="github-dark"] {  --ch-t-colorScheme: dark;--ch-t-foreground: #c9d1d9;--ch-t-background: #0d1117;--ch-t-lighter-inlineBackground: #0d1117e6;--ch-t-editor-background: #0d1117;--ch-t-editor-foreground: #c9d1d9;--ch-t-editor-lineHighlightBackground: #6e76811a;--ch-t-editor-rangeHighlightBackground: #ffffff0b;--ch-t-editor-infoForeground: #3794FF;--ch-t-editor-selectionBackground: #264F78;--ch-t-focusBorder: #1f6feb;--ch-t-tab-activeBackground: #0d1117;--ch-t-tab-activeForeground: #c9d1d9;--ch-t-tab-inactiveBackground: #010409;--ch-t-tab-inactiveForeground: #8b949e;--ch-t-tab-border: #30363d;--ch-t-tab-activeBorder: #0d1117;--ch-t-editorGroup-border: #30363d;--ch-t-editorGroupHeader-tabsBackground: #010409;--ch-t-editorLineNumber-foreground: #6e7681;--ch-t-input-background: #0d1117;--ch-t-input-foreground: #c9d1d9;--ch-t-input-border: #30363d;--ch-t-icon-foreground: #8b949e;--ch-t-sideBar-background: #010409;--ch-t-sideBar-foreground: #c9d1d9;--ch-t-sideBar-border: #30363d;--ch-t-list-activeSelectionBackground: #6e768166;--ch-t-list-activeSelectionForeground: #c9d1d9;--ch-t-list-hoverBackground: #6e76811a;--ch-t-list-hoverForeground: #c9d1d9; }'}}),"\n","\n","\n",(0,s.jsx)(n.h1,{id:"models",children:"Models"}),"\n",(0,s.jsx)(i.A,{type:"warning",title:"warning",children:(0,s.jsx)(n.p,{children:"This page may contain outdated information. It will be updated as soon as possible."})}),"\n",(0,s.jsx)(n.h2,{id:"amazon-bedrock",children:"Amazon Bedrock"}),"\n",(0,s.jsx)(n.p,{children:"This component facilitates the generation of text using the LLM (Large Language Model) model from Amazon Bedrock."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," Specifies the input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model ID (Optional):"})," Specifies the model ID to be used for text generation. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"anthropic.claude-instant-v1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"anthropic.claude-instant-v1"'}),". Available options include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-grande-instruct"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-grande-instruct"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-jumbo-instruct"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-jumbo-instruct"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-mid"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-mid"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-mid-v1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-mid-v1"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-ultra"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-ultra"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ai21.j2-ultra-v1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ai21.j2-ultra-v1"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"anthropic.claude-instant-v1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"anthropic.claude-instant-v1"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"anthropic.claude-v1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"anthropic.claude-v1"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"anthropic.claude-v2"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"anthropic.claude-v2"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"cohere.command-text-v14"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"cohere.command-text-v14"'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Credentials Profile Name (Optional):"})," Specifies the name of the credentials profile."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Region Name (Optional):"})," Specifies the region name."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Kwargs (Optional):"})," Additional keyword arguments for the model."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Endpoint URL (Optional):"})," Specifies the endpoint URL."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Streaming (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cache (Optional):"})," Specifies whether to cache the response."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(i.A,{type:"note",title:"Note",children:(0,s.jsx)("p",{children:(0,s.jsx)(n.p,{children:"Ensure that necessary credentials are provided to connect to the Amazon\nBedrock API. If connection fails, a ValueError will be raised."})})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"anthropic",children:"Anthropic"}),"\n",(0,s.jsx)(n.p,{children:"This component allows the generation of text using Anthropic Chat&Completion large language models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Name:"})," Specifies the name of the Anthropic model to be used for text generation. Available options include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"claude-2.1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"claude-2.1"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"claude-2.0"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"claude-2.0"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"claude-instant-1.2"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"claude-instant-1.2"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"claude-instant-1"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"claude-instant-1"'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Anthropic API Key:"})," Your Anthropic API key."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Tokens (Optional):"})," Specifies the maximum number of tokens to generate. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"256",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"256"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature (Optional):"})," Specifies the sampling temperature. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.7",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.7"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"API Endpoint (Optional):"})," Specifies the endpoint of the Anthropic API. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"https://api.anthropic.com"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"https://api.anthropic.com"'})," if not specified."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," Specifies the input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For detailed documentation and integration guides, please refer to the ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/chat/anthropic",children:"Anthropic Component Documentation"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"azure-openai",children:"Azure OpenAI"}),"\n",(0,s.jsx)(n.p,{children:"This component allows the generation of text using the LLM (Large Language Model) model from Azure OpenAI."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Name:"})," Specifies the name of the Azure OpenAI model to be used for text generation. Available options include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-35-turbo"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-35-turbo"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-35-turbo-16k"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-35-turbo-16k"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-35-turbo-instruct"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-35-turbo-instruct"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-4"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-4"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-4-32k"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-4-32k"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gpt-4-vision"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gpt-4-vision"'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Azure Endpoint:"})," Your Azure endpoint, including the resource. Example: ",(0,s.jsx)(n.code,{children:"https://example-resource.azure.openai.com/"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deployment Name:"})," Specifies the name of the deployment."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"API Version:"})," Specifies the version of the Azure OpenAI API to be used. Available options include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-03-15-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-03-15-preview"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-05-15"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-05-15"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-06-01-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-06-01-preview"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-07-01-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-07-01-preview"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-08-01-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-08-01-preview"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-09-01-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-09-01-preview"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"2023-12-01-preview"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"2023-12-01-preview"'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"API Key:"})," Your Azure OpenAI API key."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature (Optional):"})," Specifies the sampling temperature. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.7",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.7"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Tokens (Optional):"})," Specifies the maximum number of tokens to generate. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"1000",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"1000"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," Specifies the input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For detailed documentation and integration guides, please refer to the ",(0,s.jsx)(n.a,{href:"https://python.langchain.com/docs/integrations/llms/azure_openai",children:"Azure OpenAI Component Documentation"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"cohere",children:"Cohere"}),"\n",(0,s.jsx)(n.p,{children:"This component enables text generation using Cohere large language models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cohere API Key:"})," Your Cohere API key."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Tokens (Optional):"})," Specifies the maximum number of tokens to generate. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"256",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"256"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature (Optional):"})," Specifies the sampling temperature. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.75",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.75"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," Specifies the input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"google-generative-ai",children:"Google Generative AI"}),"\n",(0,s.jsx)(n.p,{children:"This component enables text generation using Google Generative AI."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Google API Key:"})," Your Google API key to use for the Google Generative AI."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model:"})," The name of the model to use. Supported examples are ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gemini-pro"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gemini-pro"'})," and ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"gemini-pro-vision"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"gemini-pro-vision"'}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Output Tokens (Optional):"})," The maximum number of tokens to generate."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature:"})," Run inference with this temperature. Must be in the closed interval [0.0, 1.0]."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top K (Optional):"})," Decode using top-k sampling: consider the set of top_k most probable tokens. Must be positive."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top P (Optional):"})," The maximum cumulative probability of tokens to consider when sampling."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"N (Optional):"})," Number of chat completions to generate for each prompt. Note that the API may not return the full n completions if duplicates are generated."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," The input to the model."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"hugging-face-api",children:"Hugging Face API"}),"\n",(0,s.jsx)(n.p,{children:"This component facilitates text generation using LLM models from the Hugging Face Inference API."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Endpoint URL:"})," The URL of the Hugging Face Inference API endpoint. Should be provided along with necessary authentication credentials."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Task:"})," Specifies the task for text generation. Options include ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"text2text-generation"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"text2text-generation"'}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"text-generation"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"text-generation"'}),", and ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"summarization"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"summarization"'}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"API Token:"})," The API token required for authentication with the Hugging Face Hub."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Keyword Arguments (Optional):"})," Additional keyword arguments for the model. Should be provided as a Python dictionary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," The input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"litellm-model",children:"LiteLLM Model"}),"\n",(0,s.jsxs)(n.p,{children:["Generates text using the ",(0,s.jsx)(n.code,{children:"LiteLLM"})," collection of large language models."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model name:"})," The name of the model to use. For example, ",(0,s.jsx)(n.code,{children:"gpt-3.5-turbo"}),". (Type: str)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"API key:"})," The API key to use for accessing the provider's API. (Type: str, Optional)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Provider:"}),' The provider of the API key. (Type: str, Choices: "OpenAI", "Azure", "Anthropic", "Replicate", "Cohere", "OpenRouter")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temperature:"})," Controls the randomness of the text generation. (Type: float, Default: 0.7)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model kwargs:"})," Additional keyword arguments for the model. (Type: Dict, Optional)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Top p:"})," Filter responses to keep the cumulative probability within the top p tokens. (Type: float, Optional)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Top k:"})," Filter responses to only include the top k tokens. (Type: int, Optional)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"N:"})," Number of chat completions to generate for each prompt. (Type: int, Default: 1)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Max tokens:"})," The maximum number of tokens to generate for each chat completion. (Type: int, Default: 256)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Max retries:"})," Maximum number of retries for failed requests. (Type: int, Default: 6)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verbose:"})," Whether to print verbose output. (Type: bool, Default: False)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input:"})," The input prompt for text generation. (Type: str)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stream:"})," Whether to stream the output. (Type: bool, Default: False)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System message:"})," System message to pass to the model. (Type: str, Optional)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"ollama",children:"Ollama"}),"\n",(0,s.jsx)(n.p,{children:"Generate text using Ollama Local LLMs."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Parameters"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Base URL:"})," Endpoint of the Ollama API. Defaults to '",(0,s.jsx)(n.a,{href:"http://localhost:11434",children:"http://localhost:11434"}),"' if not specified."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Name:"})," The model name to use. Refer to ",(0,s.jsx)(n.a,{href:"https://ollama.ai/library",children:"Ollama Library"})," for more models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temperature:"})," Controls the creativity of model responses. (Default: 0.8)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cache:"})," Enable or disable caching. (Default: False)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Format:"})," Specify the format of the output (e.g., json). (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Metadata:"})," Metadata to add to the run trace. (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mirostat:"})," Enable/disable Mirostat sampling for controlling perplexity. (Default: Disabled)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mirostat Eta:"})," Learning rate for Mirostat algorithm. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mirostat Tau:"})," Controls the balance between coherence and diversity of the output. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Window Size:"})," Size of the context window for generating tokens. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Number of GPUs:"})," Number of GPUs to use for computation. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Number of Threads:"})," Number of threads to use during computation. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Repeat Last N:"})," How far back the model looks to prevent repetition. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Repeat Penalty:"})," Penalty for repetitions in generated text. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TFS Z:"})," Tail free sampling value. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timeout:"})," Timeout for the request stream. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Top K:"})," Limits token selection to top K. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Top P:"})," Works together with top-k. (Default: None) (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verbose:"})," Whether to print out response text."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tags:"})," Tags to add to the run trace. (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stop Tokens:"})," List of tokens to signal the model to stop generating text. (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System:"})," System to use for generating text. (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Template:"})," Template to use for generating text. (Advanced)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input:"})," The input text."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stream:"})," Whether to stream the response."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"System Message:"})," System message to pass to the model. (Advanced)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"openai",children:"OpenAI"}),"\n",(0,s.jsx)(n.p,{children:"This component facilitates text generation using OpenAI's models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," The input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Tokens (Optional):"})," The maximum number of tokens to generate. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"256",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"256"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Kwargs (Optional):"})," Additional keyword arguments for the model. Should be provided as a nested dictionary."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Name (Optional):"})," The name of the model to use. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"4",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"1106",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"preview",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"gpt-4-1106-preview"}),". Supported options include: ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"4",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"turbo",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"preview",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"gpt-4-turbo-preview"}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"4",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"0125",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"preview",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"gpt-4-0125-preview"}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"4",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"1106",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"preview",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"gpt-4-1106-preview"}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"4",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"vision",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"preview",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"gpt-4-vision-preview"}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"3.5",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"turbo",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"0125",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"gpt-3.5-turbo-0125"}),", ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"gpt",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"3.5",props:{style:{color:"#79C0FF"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"turbo",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"1106",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"gpt-3.5-turbo-1106"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI API Base (Optional):"})," The base URL of the OpenAI API. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"https",props:{style:{color:"#FFA657"}}},{content:":",props:{style:{color:"#C9D1D9"}}},{content:"//api.openai.com/v1",props:{style:{color:"#8B949E"}}}]}],lang:"jsx"},children:"https://api.openai.com/v1"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI API Key (Optional):"})," The API key for accessing the OpenAI API."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature:"})," Controls the creativity of model responses. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.7",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.7"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," System message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"qianfan",children:"Qianfan"}),"\n",(0,s.jsx)(n.p,{children:"This component facilitates the generation of text using Baidu Qianfan chat models."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Name:"})," Specifies the name of the Qianfan chat model to be used for text generation. Available options include:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ERNIE-Bot"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ERNIE-Bot"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ERNIE-Bot-turbo"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ERNIE-Bot-turbo"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"BLOOMZ-7B"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"BLOOMZ-7B"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"Llama-2-7b-chat"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"Llama-2-7b-chat"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"Llama-2-13b-chat"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"Llama-2-13b-chat"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"Llama-2-70b-chat"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"Llama-2-70b-chat"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"Qianfan-BLOOMZ-7B-compressed"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"Qianfan-BLOOMZ-7B-compressed"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"Qianfan-Chinese-Llama-2-7B"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"Qianfan-Chinese-Llama-2-7B"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"ChatGLM2-6B-32K"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"ChatGLM2-6B-32K"'})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:'"AquilaChat-7B"',props:{style:{color:"#A5D6FF"}}}]}],lang:"jsx"},children:'"AquilaChat-7B"'})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Qianfan Ak:"})," Your Baidu Qianfan access key, obtainable from ",(0,s.jsx)(n.a,{href:"https://cloud.baidu.com/product/wenxinworkshop",children:"here"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Qianfan Sk:"})," Your Baidu Qianfan secret key, obtainable from ",(0,s.jsx)(n.a,{href:"https://cloud.baidu.com/product/wenxinworkshop",children:"here"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top p (Optional):"})," Model parameter. Specifies the top-p value. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.8",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.8"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature (Optional):"})," Model parameter. Specifies the sampling temperature. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.95",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.95"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Penalty Score (Optional):"})," Model parameter. Specifies the penalty score. Only supported in ERNIE-Bot and ERNIE-Bot-turbo models. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"1.0",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"1.0"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Endpoint (Optional):"})," Endpoint of the Qianfan LLM, required if custom model is used."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," Specifies the input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," A system message to pass to the model."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"vertex-ai",children:"Vertex AI"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"ChatVertexAI"})," is a component for generating text using Vertex AI Chat large language models API."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Params"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Credentials:"})," The JSON file containing the credentials for accessing the Vertex AI Chat API."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Project:"})," The name of the project associated with the Vertex AI Chat API."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Examples (Optional):"})," List of examples to provide context for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Location:"})," The location of the Vertex AI Chat API service. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"us",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"central1",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"us-central1"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Max Output Tokens:"})," The maximum number of tokens to generate. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"128",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"128"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Model Name:"})," The name of the model to use. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"chat",props:{style:{color:"#C9D1D9"}}},{content:"-",props:{style:{color:"#FF7B72"}}},{content:"bison",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"chat-bison"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Temperature:"})," Controls the creativity of model responses. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.0",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.0"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Input Value:"})," The input text for text generation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top K:"})," Limits token selection to top K. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"40",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"40"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Top P:"})," Works together with top-k. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"0.95",props:{style:{color:"#79C0FF"}}}]}],lang:"jsx"},children:"0.95"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Verbose:"})," Whether to print out response text. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Stream (Optional):"})," Specifies whether to stream the response from the model. Defaults to ",(0,s.jsx)(h.InlineCode,{codeConfig:p,code:{lines:[{tokens:[{content:"False",props:{style:{color:"#C9D1D9"}}}]}],lang:"jsx"},children:"False"}),"."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"System Message (Optional):"})," System message to pass to the model."]}),"\n"]}),"\n"]})]})}const g=function(e={}){const{wrapper:n}=Object.assign({},(0,t.RP)(),e.components);return n?(0,s.jsx)(n,Object.assign({},e,{children:(0,s.jsx)(j,e)})):j(e)};function u(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}}}]);